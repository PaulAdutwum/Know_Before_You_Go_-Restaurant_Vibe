# ğŸ”‘ Complete API & Sentiment Guide

## ğŸ“‹ What You Need (Checklist)

### âœ… **Required Components**

| Component | Purpose | Status | Notes |
|-----------|---------|--------|-------|
| **PostgreSQL** | Store scraped reviews | âš ï¸ Setup needed | See DATABASE_SETUP_GUIDE.md |
| **Redis** | Background job queue | âš ï¸ Install needed | `brew install redis` |
| **Google Places API** | Find restaurants | âœ… Already have | Already in .env |
| **Selenium/Chrome** | Scrape Google Maps | âœ… Auto-install | webdriver-manager handles it |

### ğŸ”„ **Optional Components**

| Component | Purpose | Status | Notes |
|-----------|---------|--------|-------|
| **Reddit API** | Supplementary reviews | âš ï¸ Optional | Free from reddit.com/prefs/apps |

---

## ğŸ—„ï¸ **Database Setup (Required)**

### **Option 1: PostgreSQL (Recommended)**

```bash
# 1. Install PostgreSQL
brew install postgresql@14
brew services start postgresql@14

# 2. Create database
createdb vibefinder

# 3. Initialize tables
cd backend
source venv/bin/activate
python setup_db.py
```

**Verify:**
```bash
psql vibefinder
\dt  # Should show: restaurants, reviews, scraping_jobs
\q
```

### **Option 2: SQLite (Simpler)**

If PostgreSQL is too complex, use SQLite:

**Edit `backend/.env`:**
```bash
DATABASE_URL=sqlite:///./vibefinder.db
```

**Initialize:**
```bash
cd backend
python setup_db.py
```

SQLite creates a file `vibefinder.db` in the backend folder.

---

## ğŸ”„ **Redis Setup (Required for Background Jobs)**

```bash
# macOS
brew install redis
brew services start redis

# Verify it's running
redis-cli ping
# Should return: PONG
```

**Why needed:** Redis manages the background scraping job queue.

---

## ğŸ”‘ **API Keys Breakdown**

### **1. Google Places API** âœ… Already Configured

**What it does:**
- Finds restaurants by location
- Gets basic info: name, address, rating
- Gets place_id for scraping

**Status:** âœ… You already have this
```bash
GOOGLE_PLACES_API_KEY=AIzaSyCvGpQYjkc_laF27tL8z_r6EpeQ0Q6U6VI
```

**No additional setup needed!**

---

### **2. Google Maps Scraping** âœ… No API Needed

**What it does:**
- Scrapes 100+ reviews from Google Maps website
- Uses Selenium (browser automation)
- Gets full review text, ratings, dates

**Status:** âœ… Already works - NO API key needed!

**How:** Acts like a browser visiting google.com/maps

**Requirements:**
- Chrome browser (auto-installed)
- ChromeDriver (auto-installed via webdriver-manager)

---

### **3. Reddit API** âš ï¸ Optional

**What it does:**
- Searches Reddit for restaurant mentions
- Adds supplementary sentiment data
- Fully legal via official API

**Status:** âš ï¸ Not configured yet (optional)

**Setup (5 minutes):**

1. Go to: https://www.reddit.com/prefs/apps
2. Click "Create App"
3. Fill in:
   - Name: `VibeFinder`
   - Type: **script**
   - Redirect URI: `http://localhost:8080`
4. Copy credentials

**Add to `backend/.env`:**
```bash
REDDIT_CLIENT_ID=your_id_here
REDDIT_CLIENT_SECRET=your_secret_here
REDDIT_USER_AGENT=VibeFinder/1.0
```

**What happens without Reddit:**
- System still works perfectly
- Just skips Reddit mentions
- Logs a warning (not an error)

---

## ğŸ§  **How Sentiment Analysis Works**

### **VADER Sentiment Analyzer**

**Technology:** VADER (Valence Aware Dictionary and sEntiment Reasoner)

**Why VADER:**
- âœ… Perfect for social media & reviews
- âœ… Understands slang ("awesome!", "meh", "sucks")
- âœ… Handles emojis and punctuation
- âœ… No training needed (pre-built)
- âœ… Fast and accurate

**How it works:**

#### **Step 1: Analyze Each Review**
```python
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

review = "The pizza was absolutely amazing! Best I've ever had!"
scores = analyzer.polarity_scores(review)

# Output:
{
  'neg': 0.0,      # Negative score (0%)
  'neu': 0.323,    # Neutral score (32%)
  'pos': 0.677,    # Positive score (68%)
  'compound': 0.87 # Overall score (-1 to +1)
}
```

#### **Step 2: Average All Reviews**
```python
# For restaurant with 100 reviews:
all_scores = [0.87, 0.92, -0.34, 0.76, 0.81, ...]
average = sum(all_scores) / len(all_scores)
# average = 0.72
```

#### **Step 3: Convert to Percentage**
```python
# Convert from [-1, 1] to [0, 100]
percentage = int((average + 1) / 2 * 100)
# percentage = 86

# Categorize
if average >= 0.5:
    label = "Very Positive"
elif average >= 0.05:
    label = "Positive"
elif average >= -0.05:
    label = "Neutral"
else:
    label = "Negative"

# Result: "86% Very Positive"
```

### **Real Example:**

**Input:** 127 reviews from "Joe's Pizza"

**Sample Reviews:**
- "Best pizza in town! ğŸ•" â†’ `compound: 0.89`
- "Pretty good, nice crust" â†’ `compound: 0.65`
- "Meh, nothing special" â†’ `compound: -0.28`
- "Amazing sauce!" â†’ `compound: 0.82`

**Process:**
```
Average of 127 compound scores: 0.64
Convert to percentage: (0.64 + 1) / 2 * 100 = 82%
Categorize: 0.64 >= 0.05 = "Positive"
Result: "82% Positive"
```

**Display on UI:**
```
âœ… 82% Positive (shown in green)
```

---

## ğŸ¯ **Other ML Models**

### **1. Topic Modeling (Vibe Detection)**

**Technology:** LDA (Latent Dirichlet Allocation)

**What it does:** Finds hidden patterns in reviews

**Example:**
```
Reviews mention: "loud", "music", "friends", "bar", "group"
â†“
Model detects Topic: Social/Loud atmosphere
â†“
Maps to: #Loud, #GoodForGroups
```

**How it works:**
```python
from sklearn.decomposition import LatentDirichletAllocation

# Discover 3 topics in reviews
lda = LatentDirichletAllocation(n_components=3)
topics = lda.fit_transform(review_text_vectors)

# Topic 1: [loud, music, bar, friends] â†’ #Loud
# Topic 2: [romantic, quiet, date] â†’ #Romantic
# Topic 3: [family, kids, friendly] â†’ #FamilyFriendly
```

### **2. Keyword Extraction (Dishes & Complaints)**

**Technology:** TF-IDF (Term Frequency-Inverse Document Frequency)

**What it does:** Finds important words/phrases

**Example - Dishes:**
```
Reviews frequently mention:
- "spicy rigatoni" (mentioned 23 times)
- "garlic knots" (mentioned 18 times)
- "margherita" (mentioned 15 times)
â†“
Extract as: Must-Try Dishes
```

**Example - Complaints:**
```
Negative sentiment + phrases:
- "slow service" (mentioned 8 times)
- "too expensive" (mentioned 5 times)
â†“
Extract as: Common Complaints
```

**How it works:**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1, 3))
tfidf_matrix = vectorizer.fit_transform(reviews)

# Get most important n-grams
top_dishes = get_top_n_grams(tfidf_matrix, n=5)
# ["spicy rigatoni", "garlic knots", "margherita pizza", ...]
```

---

## ğŸ”„ **Complete Data Flow**

```
1. User Searches "Pizza Boston"
         â†“
2. Google Places API finds restaurants
   â†’ Returns: name, address, place_id
         â†“
3. Check Database
   â”œâ”€ Reviews cached? â†’ Use them (instant!)
   â””â”€ No cache? â†’ Queue background job
         â†“
4. Background Job (Celery Worker)
   â”œâ”€ Google Maps Scraper: 100+ reviews
   â”‚  â””â”€ Uses Selenium (no API needed)
   â”œâ”€ Reddit Scraper: Mentions (optional)
   â”‚  â””â”€ Uses Reddit API
   â””â”€ Save to PostgreSQL
         â†“
5. ML Processing (Automatic)
   â”œâ”€ VADER: Sentiment analysis
   â”‚  â†’ "82% Positive"
   â”œâ”€ LDA: Topic modeling
   â”‚  â†’ "#Loud", "#GoodForGroups"
   â”œâ”€ TF-IDF: Dish extraction
   â”‚  â†’ ["Spicy Rigatoni", "Garlic Knots"]
   â””â”€ TF-IDF: Complaint detection
      â†’ ["Slow service on weekends"]
         â†“
6. Save ML Results to Database
         â†“
7. Next Search: Instant (uses cache!)
```

---

## âœ… **Setup Checklist**

```bash
# 1. Install PostgreSQL
brew install postgresql@14
brew services start postgresql@14
createdb vibefinder

# 2. Install Redis
brew install redis
brew services start redis

# 3. Install Python dependencies
cd backend
source venv/bin/activate
pip install -r requirements.txt

# 4. Initialize database
python setup_db.py

# 5. Verify setup
redis-cli ping        # Should return: PONG
psql vibefinder -c "\dt"  # Should show tables

# 6. Optional: Add Reddit API (skip if you want)
# Edit .env with Reddit credentials

# 7. Run the system (3 terminals)
# Terminal 1: uvicorn app.main:app --reload
# Terminal 2: celery -A celery_worker worker --loglevel=info
# Terminal 3: cd ../frontend && npm run dev
```

---

## ğŸ“ **Summary**

### **APIs Needed:**

| API | Status | Notes |
|-----|--------|-------|
| Google Places | âœ… Already have | For finding restaurants |
| Google Maps | âœ… No API needed | Scraping with Selenium |
| Reddit | âš ï¸ Optional | Get from reddit.com/prefs/apps |

### **Infrastructure Needed:**

| Component | Status | Command |
|-----------|--------|---------|
| PostgreSQL | âš ï¸ Setup | `brew install postgresql@14` |
| Redis | âš ï¸ Setup | `brew install redis` |

### **Sentiment Analysis:**

- âœ… **VADER** - Pre-built, no API needed
- âœ… **LDA** - Built into scikit-learn
- âœ… **TF-IDF** - Built into scikit-learn
- âœ… All models work offline after pip install

---

## ğŸš€ **Quick Start Commands**

```bash
# Install everything
brew install postgresql@14 redis
brew services start postgresql@14 redis
createdb vibefinder

# Setup backend
cd backend
source venv/bin/activate
pip install -r requirements.txt
python setup_db.py

# Run (3 terminals)
# Terminal 1: uvicorn app.main:app --reload
# Terminal 2: celery -A celery_worker worker --loglevel=info
# Terminal 3: cd ../frontend && npm run dev
```

**That's it! No additional APIs or licenses needed!** ğŸ‰

